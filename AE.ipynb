{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Evaluation Instructions: #506   Combining Static and Dynamic Code Information for Software Vulnerability Prediction\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook provides a small-scale demo to showcase the program representation, vulnerability detection, and prediction of vulnerability detection discussed in the paper.\n",
    "\n",
    "The main results of our CCS 2023 paper involve comparing the performance of our vulnerability detection with prior machine learning-based approaches. The evaluation presented in our paper was conducted on a much larger dataset and for a longer duration. The intention of this notebook is to provide minimal working examples that can be evaluated within a reasonable time frame.\n",
    "\n",
    "## Instructions for Experimental Workflow:\n",
    "\n",
    "Before you start, please first make a copy of the notebook by going to the landing page. Then select the checkbox next to the notebook titled *main.ipynb*, then click \"**Duplicate**\".\n",
    "\n",
    "Click the name of the newly created Jupyter Notebook, e.g. **AE-Copy1.ipynb**. Next, select \"**Kernel**\" > \"**Restart & Clear Output**\". Then, repeatedly press the play button (the tooltip is \"run cell, select below\") to step through each cell of the notebook.\n",
    "\n",
    "Alternatively, select each cell in turn and use \"**Cell**\"> \"**Run Cell**\" from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure all previous cells have been executed.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "**Some cells can take more than half an hour to complete; please wait for the results until step to the next cell.** \n",
    "\n",
    "High load can lead to a long wait for results. This may occur if multiple reviewers are simultaneously trying to generate results. \n",
    "\n",
    "The experiments are customisable as the code provided in the Jupyter Notebook can be edited on the spot. Simply type your changes into the code blocks and re-run using **Cell > Run Cells** from the menu.\n",
    "\n",
    "## Links to The Paper\n",
    "\n",
    "For each step, we note the section number of the submitted version where the relevant technique is described or data is presented.\n",
    "\n",
    "The main results are presented in Figures 9-12 of the submitted paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: The Concoction Model Architecture\n",
    "\n",
    "This demo corresponds to the architecture given in Figure 3. Note that This is a small-scale demo for vulnerability detection. The full-scale evaluation used in the paper takes over 24 hours to run.\n",
    "\n",
    "## Step 1. Program representation\n",
    "\n",
    "The program representation component maps the input source code and dynamic symbolic execution traces of the target function into a numerical embedding vector.\n",
    "\n",
    "#### *Static representation model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.666390500Z",
     "start_time": "2023-05-08T13:43:09.658388600Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "2023-10-08 13:23:42,036 - INFO - project : /home/feature/static/test_pro\n",
      "2023-10-08 13:23:42,036 - INFO - start to get function segmentation... \n",
      "finished:0.00%   Batch 1\n",
      "2023-10-08 13:23:43,303 - INFO - start to get graphRelation by joern... \n",
      "joern_relationgood.py over...\n",
      "2023-10-08 13:24:03,708 - INFO - start to get seven edges...\n",
      "2023-10-08 13:24:06,910 - INFO - start to get cfgpath  by joern and concate... \n",
      "Processing getcfg jasper.c-testtttt.c: 100%|██████| 2/2 [00:24<00:00, 12.34s/it]\n",
      "============================================================================================================\n",
      "2023-10-08 13:24:32,523 - INFO - extracted static feature stored in : /home/feature/static/github/static/\n"
     ]
    }
   ],
   "source": [
    "#  Extract Program Information (To show static code information like sec 3.3.1.)\n",
    "import os\n",
    "current=os.getcwd()\n",
    "dir=os.path.join(current,'feature/static/github')\n",
    "projectPath=os.path.join(current,'example/jasper-version-1.900.1')\n",
    "!cd $dir && python GetStatic.py $projectPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "dirs: 276it [00:02, 95.13it/s] \n",
      "10/10/2023 08:18:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 3, distributed training: False, 16-bits training: False\n",
      "10/10/2023 08:18:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(output_dir=/home/concoction/pretrainedModel/staticRepresentation/trainedModel, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Oct10_08-18-27_10290d7ddecb, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/home/concoction/pretrainedModel/staticRepresentation/trainedModel, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=3)\n",
      "[INFO|configuration_utils.py:443] 2023-10-10 08:18:27,240 >> loading configuration file graphcodebert-base/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-10-10 08:18:27,241 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:443] 2023-10-10 08:18:27,241 >> loading configuration file graphcodebert-base/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-10-10 08:18:27,242 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2023-10-10 08:18:27,242 >> Model name 'graphcodebert-base' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'graphcodebert-base' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1718] 2023-10-10 08:18:27,242 >> Didn't find file graphcodebert-base/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1718] 2023-10-10 08:18:27,242 >> Didn't find file graphcodebert-base/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file graphcodebert-base/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file graphcodebert-base/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file graphcodebert-base/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-10-10 08:18:27,242 >> loading file graphcodebert-base/tokenizer_config.json\n",
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:1025] 2023-10-10 08:18:27,363 >> loading weights file graphcodebert-base/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1143] 2023-10-10 08:18:30,810 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2023-10-10 08:18:30,810 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at graphcodebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "[INFO|language_modeling.py:134] 2023-10-10 08:18:30,812 >> Creating features from dataset file at /home/concoction/data/output_static.txt\n",
      "[INFO|trainer.py:791] 2023-10-10 08:18:45,165 >> ***** Running training *****\n",
      "[INFO|trainer.py:792] 2023-10-10 08:18:45,165 >>   Num examples = 276\n",
      "[INFO|trainer.py:793] 2023-10-10 08:18:45,165 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:794] 2023-10-10 08:18:45,165 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:795] 2023-10-10 08:18:45,165 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:796] 2023-10-10 08:18:45,165 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:797] 2023-10-10 08:18:45,165 >>   Total optimization steps = 36\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"train.py\", line 361, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 325, in main\n",
      "    trainer.train(model_path=model_path)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/trainer.py\", line 888, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/trainer.py\", line 1250, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/trainer.py\", line 1277, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 160, in forward\n",
      "    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in replicate\n",
      "    return replicate(module, device_ids, not torch.is_grad_enabled())\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\", line 88, in replicate\n",
      "    param_copies = _broadcast_coalesced_reshape(params, devices, detach)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\", line 71, in _broadcast_coalesced_reshape\n",
      "    tensor_copies = Broadcast.apply(devices, *tensors)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 22, in forward\n",
      "    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/parallel/comm.py\", line 56, in broadcast_coalesced\n",
      "    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)\n",
      "RuntimeError: NCCL Error 2: unhandled system error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                    | 0/36 [00:04<?, ?it/s]\r\n"
     ]
    }
   ],
   "source": [
    "#  Pretrain Representation Models (Here we use Graphcodebert like sec 3.4)\n",
    "#preprocess\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/dataset')\n",
    "outputPath=os.path.join(current,'concoction/data/output_static.txt')\n",
    "dir=os.path.join(current,'concoction/pretrainedModel/staticRepresentation')\n",
    "!cd $dir && python preprocess.py --data_path $path --output_path $outputPath\n",
    "#pretrain\n",
    "storedPath=os.path.join(os.getcwd(),'concoction/pretrainedModel/staticRepresentation/trainedModel')\n",
    "!cd $dir && python train.py --model_name_or_path graphcodebert-base --train_data_file $outputPath --per_device_train_batch_size 8 --do_train --output_dir $storedPath --mlm --overwrite_output_dir --line_by_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/concoction/pretrainedModel/staticRepresentation/trainedModel and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2573,  0.2688, -0.1238,  ..., -0.1123, -0.1479,  0.3731],\n",
      "         [ 0.5010, -0.2858,  0.6845,  ...,  0.2044,  0.8592,  0.0892],\n",
      "         [ 0.1018,  0.6084,  0.0398,  ...,  0.5097, -0.3028,  0.1666],\n",
      "         ...,\n",
      "         [ 0.2921,  0.1633,  0.7146,  ...,  0.7883,  0.5627, -0.1219],\n",
      "         [ 0.2624,  0.6262, -0.0925,  ...,  1.7611, -0.5254,  0.1670],\n",
      "         [ 0.2574,  0.2688, -0.1238,  ..., -0.1123, -0.1481,  0.3735]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (like the example in https://github.com/microsoft/CodeBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "storedPath=os.path.join(os.getcwd(),'concoction/pretrainedModel/staticRepresentation/trainedModel')\n",
    "tokenizer = AutoTokenizer.from_pretrained(storedPath)\n",
    "model = AutoModel.from_pretrained(storedPath)\n",
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.eos_token]\n",
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### *Dynamic representation model*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.678393300Z",
     "start_time": "2023-05-08T13:43:09.665390300Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Extract Program Information (To show dynamic code information like sec 3.3.2.)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "dir=os.path.join(current,'feature/dynamic')\n",
    "shell_file=os.path.join(dir,'main.sh')\n",
    "project_path=os.path.join(current,'jasper-version-1.900.1/jasper-version-1.900.1')\n",
    "dependency_txt=os.path.join(dir,'before_insert.txt')\n",
    "compile_txt=os.path.join(dir,'compile.txt')\n",
    "kleecmd_txt=os.path.join(dir,'do.txt')\n",
    "!cd $dir && sh $shell_file $project_path $dependency_txt $compile_txt $kleecmd_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "dirs: 12it [00:00, 31.31it/s]\n",
      "cpu\n",
      "device\n",
      "09/20/2023 08:41:12 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0 distributed training: False, 16-bits training: False\n",
      "09/20/2023 08:41:12 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='./result', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Sep20_08-41-12_10290d7ddecb', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=2, dataloader_num_workers=0, past_index=-1, run_name='./result', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)\n",
      "09/20/2023 08:41:13 - WARNING - datasets.builder -   Using custom data configuration default-6ddbf745f050ba84\n",
      "Downloading and preparing dataset text/default to ./data/text/default-6ddbf745f050ba84/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 3833.92it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 769.74it/s]\n",
      "Dataset text downloaded and prepared to ./data/text/default-6ddbf745f050ba84/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 30.15it/s]\n",
      "[INFO|configuration_utils.py:443] 2023-09-20 08:41:13,429 >> loading configuration file bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-20 08:41:13,430 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:443] 2023-09-20 08:41:13,432 >> loading configuration file bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-20 08:41:13,433 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1766] 2023-09-20 08:41:33,499 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1766] 2023-09-20 08:41:33,500 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|modeling_utils.py:1025] 2023-09-20 08:41:33,608 >> loading weights file bert-base-uncased/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:1135] 2023-09-20 08:41:36,267 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1146] 2023-09-20 08:41:36,268 >> Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.28ba/s]\n",
      "[INFO|trainer.py:442] 2023-09-20 08:41:36,372 >> The following columns in the training set don't have a corresponding argument in `BertForCL.forward` and have been ignored: .\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -   ***** Running training *****\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Num examples = 11\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Num Epochs = 1\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Instantaneous batch size per device = 32\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Gradient Accumulation steps = 1\n",
      "09/20/2023 08:41:36 - INFO - utils.trainers -     Total optimization steps = 1\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]09/20/2023 08:41:37 - INFO - utils.trainers -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1.3555, 'train_samples_per_second': 0.738, 'epoch': 1.0}      \n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "[INFO|trainer.py:1344] 2023-09-20 08:41:37,736 >> Saving model checkpoint to ./result\n",
      "[INFO|configuration_utils.py:300] 2023-09-20 08:41:37,796 >> Configuration saved in ./result/config.json\n",
      "[INFO|modeling_utils.py:817] 2023-09-20 08:41:42,307 >> Model weights saved in ./result/pytorch_model.bin\n",
      "09/20/2023 08:41:42 - INFO - __main__ -   ***** Train results *****\n",
      "09/20/2023 08:41:42 - INFO - __main__ -     epoch = 1.0\n",
      "09/20/2023 08:41:42 - INFO - __main__ -     train_runtime = 1.3555\n",
      "09/20/2023 08:41:42 - INFO - __main__ -     train_samples_per_second = 0.738\n"
     ]
    }
   ],
   "source": [
    "#  Pretrain Representation Models (Here we use Simcse like sec 3.4)\n",
    "#preprocess\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/feature')\n",
    "outputPath=os.path.join(current,'concoction/data/output_dynamic.txt')\n",
    "dir=os.path.join(current,'concoction/pretrainedModel/dynamicRepresentation')\n",
    "!cd $dir && python preprocess.py --data_path $path --output_path $outputPath\n",
    "#training\n",
    "! cd $dir && python train.py --model_name_or_path bert-base-uncased     --train_file $outputPath   --output_dir ./result    --num_train_epochs 1     --per_device_train_batch_size 32     --learning_rate 3e-5     --max_seq_length 32      --metric_for_best_model stsb_spearman  --load_best_model_at_end     --eval_steps 2     --pooler_type cls     --mlp_only_train     --overwrite_output_dir     --temp 0.05     --do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/concoction/pretrainedModel/dynamicRepresentation/result and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3348, -0.7194, -0.3097,  ..., -0.6366, -0.3357,  1.0084],\n",
      "         [-0.0132, -0.1685,  0.1986,  ..., -0.1092, -0.2491,  0.3447],\n",
      "         [-0.3284, -0.2352,  0.0701,  ..., -0.6065, -0.3233,  0.3300],\n",
      "         ...,\n",
      "         [-0.2394, -0.2376,  0.3637,  ..., -0.2333,  0.3245,  0.2509],\n",
      "         [ 0.2682,  0.0495,  0.9866,  ..., -0.1620, -0.9476,  0.1886],\n",
      "         [ 0.7464,  0.2502,  0.0631,  ..., -1.2616,  0.3442,  0.3946]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (like the example in https://github.com/microsoft/CodeBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch,os\n",
    "storedPath=os.path.join(os.getcwd(),'concoction/pretrainedModel/dynamicRepresentation/result')\n",
    "tokenizer = AutoTokenizer.from_pretrained(storedPath)\n",
    "model = AutoModel.from_pretrained(storedPath)\n",
    "\n",
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens\n",
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2. Vulnerability Detection\n",
    "\n",
    " Concoction’s detection component takes the joint embedding as input to predict the presence of vulnerabilities.  Our current implementation only identifies whether a function may contain a vulnerability or bug and does not specify the type of vulnerability. Here we use SARD benchmarks.\n",
    "\n",
    "**approximate runtime ~ 30 minutes (please wait before moving to the next cell)**\n",
    "\n",
    "#### *Dynamic representation model*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION\n",
      "0\n",
      "2023-09-12 06:45:51,369 : Load data from exist npy\n",
      "[2023-09-12 06:45:51] INFO (root/MainThread) Load data from exist npy\n",
      "total train files is  146\n",
      "2023-09-12 06:45:51,529 : Computing embedding for train\n",
      "[2023-09-12 06:45:51] INFO (root/MainThread) Computing embedding for train\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:07<00:00,  2.52s/it]\n",
      "2023-09-12 06:45:59,099 : Computed train embeddings\n",
      "[2023-09-12 06:45:59] INFO (root/MainThread) Computed train embeddings\n",
      "2023-09-12 06:45:59,099 : Computing embedding for dev\n",
      "[2023-09-12 06:45:59] INFO (root/MainThread) Computing embedding for dev\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.49s/it]\n",
      "2023-09-12 06:46:01,588 : Computed dev embeddings\n",
      "[2023-09-12 06:46:01] INFO (root/MainThread) Computed dev embeddings\n",
      "2023-09-12 06:46:01,589 : Computing embedding for test\n",
      "[2023-09-12 06:46:01] INFO (root/MainThread) Computing embedding for test\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "2023-09-12 06:46:03,945 : Computed test embeddings\n",
      "[2023-09-12 06:46:03] INFO (root/MainThread) Computed test embeddings\n",
      "2023-09-12 06:46:03,945 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "[2023-09-12 06:46:03] INFO (root/MainThread) Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2023-09-12 06:46:04,021 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,039 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,058 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,077 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,095 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,114 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,132 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,151 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,151 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,189 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,208 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,226 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,244 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,263 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,281 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,299 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,318 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,318 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,342 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,360 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,378 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,397 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-12 06:46:04,415 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,434 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,452 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,470 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,470 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,494 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,513 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,531 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,549 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,568 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,586 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,604 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,623 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,623 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,627 : [('reg:1e-05', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.0001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.01', array([0.97959184, 0.95999998, 1.        , 0.9795918 ]))]\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) [('reg:1e-05', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.0001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.01', array([0.97959184, 0.95999998, 1.        , 0.9795918 ]))]\n",
      "2023-09-12 06:46:04,629 : Validation : f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Validation : f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,629 : Evaluating...\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Evaluating...\n",
      "2023-09-12 06:46:04,648 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,667 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,685 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,703 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,722 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,740 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,758 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,777 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,777 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,778 : model clf saved to./f1_0.9795917957735685_2023-09-12.h5...\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) model clf saved to./f1_0.9795917957735685_2023-09-12.h5...\n",
      "2023-09-12 06:46:04,783 : Test : f1 = 0.8823529294205378, precision = 0.7894737124443054,recall = 1.0,accuracy = 0.9183673469387755\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Test : f1 = 0.8823529294205378, precision = 0.7894737124443054,recall = 1.0,accuracy = 0.9183673469387755\n",
      "this is the data path: /home/model/data/sard/cwe416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#  Train detection Model (Here we use Simcse like sec 3.5.2)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/feature')\n",
    "dir=os.path.join(current,'concoction/detectionModel')\n",
    "!cd $dir && python evaluation_bug.py --path_to_data /home/model/data/sard/cwe416 --mode train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "{'nhid': 2, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 200}\n",
      "0\n",
      "2023-09-20 08:58:23,236 : Load data from exist npy\n",
      "[2023-09-20 08:58:23] INFO (root/MainThread) Load data from exist npy\n",
      "100%|███████████████████████████████████████████| 32/32 [01:39<00:00,  3.10s/it]\n",
      "2023-09-20 09:00:02,732 : Computed data embeddings\n",
      "[2023-09-20 09:00:02] INFO (root/MainThread) Computed data embeddings\n",
      "2023-09-20 09:00:02,733 : model load from/home/concoction/detectionModel/f1_0.9795917957735685_2023-09-12.h5\n",
      "[2023-09-20 09:00:02] INFO (root/MainThread) model load from/home/concoction/detectionModel/f1_0.9795917957735685_2023-09-12.h5\n",
      "2023-09-20 09:00:02,961 : prediction : f1 = 0.6939571400480062, precision = 0.5313432812690735,recall = 1.0,accuracy = 0.5389133627019089\n",
      "[2023-09-20 09:00:02] INFO (root/MainThread) prediction : f1 = 0.6939571400480062, precision = 0.5313432812690735,recall = 1.0,accuracy = 0.5389133627019089\n",
      "this is the data path: /home/concoction/data/dataset\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (Load trained model and test on test case)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/dataset')\n",
    "dir=os.path.join(current,'concoction/detectionModel')\n",
    "!cd $dir && python evaluation_bug.py  --model_to_load $dir/f1_0.9795917957735685_2023-09-12.h5 --path_to_data $path  --mode test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Deployment\n",
    "\n",
    "This demo shows how to deploy our trained model on a real world project. Here we apply the xx as our test project.\n",
    "\n",
    "#### *Path Selection for Symbolic Execution*:\n",
    "After training the end-to-end model, we develop a path selection component to automatically select a subset of important paths whose dynamic traces are likely to improve prediction accuracy during deployment.\n",
    "\n",
    "*approximate runtime ~ 30 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.709400100Z",
     "start_time": "2023-05-08T13:43:09.695397900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "preprocess data........\n",
      "dirs: 0it [00:00, ?it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccgetsint32.c.txt\n",
      "dirs: 1it [00:00,  7.22it/s]/home/concoction/data/dataset/jas_malloc.c-jas_free.c.txt\n",
      "/home/concoction/data/dataset/jas_icc.c-jas_iccgetuint.c.txt\n",
      "dirs: 3it [00:00,  9.78it/s]/home/concoction/data/dataset/jpc_mqenc.c-jpc_mqenc_codelps.c.txt\n",
      "/home/concoction/data/dataset/jpc_enc.c-prc_create.c.txt\n",
      "dirs: 5it [00:00, 11.89it/s]/home/concoction/data/dataset/jp2_cod.c-jp2_box_create.c.txt\n",
      "/home/concoction/data/dataset/jas_image.c-jas_image_writecmpt.c.txt\n",
      "dirs: 7it [00:08,  1.70s/it]/home/concoction/data/dataset/jpc_qmfb.c-jpc_qmfb_split_row.c.txt\n",
      "dirs: 8it [00:09,  1.53s/it]/home/concoction/data/dataset/jas_stream.c-jas_stream_flushbuf.c.txt\n",
      "dirs: 9it [00:09,  1.23s/it]/home/concoction/data/dataset/jas_tmr.c-jas_tmr_start.c.txt\n",
      "/home/concoction/data/dataset/jpc_mqenc.c-jpc_mqenc_flush.c.txt\n",
      "dirs: 11it [00:09,  1.26it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_siz_destroyparms.c.txt\n",
      "/home/concoction/data/dataset/jpc_tagtree.c-jpc_tagtree_create.c.txt\n",
      "dirs: 13it [00:10,  1.78it/s]/home/concoction/data/dataset/jpc_t1cod.c-jpc_initluts.c.txt\n",
      "dirs: 14it [00:13,  1.12s/it]/home/concoction/data/dataset/jas_stream.c-jas_stream_ungetc.c.txt\n",
      "dirs: 15it [00:13,  1.11it/s]/home/concoction/data/dataset/jas_stream.c-jas_stream_copy.c.txt\n",
      "dirs: 16it [00:14,  1.19it/s]/home/concoction/data/dataset/jas_icc.c-jas_icccurv_destroy.c.txt\n",
      "/home/concoction/data/dataset/jas_icc.c-jas_iccprof_setattr.c.txt\n",
      "dirs: 18it [00:14,  1.77it/s]/home/concoction/data/dataset/jas_tvp.c-jas_tvparser_gettag.c.txt\n",
      "/home/concoction/data/dataset/pnm_cod.c-pnm_maxvaltodepth.c.txt\n",
      "dirs: 20it [00:14,  2.64it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmprof_createfromiccprof.c.txt\n",
      "dirs: 21it [00:16,  1.46it/s]/home/concoction/data/dataset/jas_tvp.c-jas_tvparser_create.c.txt\n",
      "dirs: 22it [00:16,  1.80it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccattrtab_destroy.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-jas_stream_flush.c.txt\n",
      "dirs: 24it [00:16,  2.70it/s]/home/concoction/data/dataset/jas_cm.c-icctoclrspc.c.txt\n",
      "dirs: 25it [00:17,  2.82it/s]/home/concoction/data/dataset/jas_stream.c-jas_stream_initbuf.c.txt\n",
      "dirs: 26it [00:17,  3.21it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_cstate_create.c.txt\n",
      "/home/concoction/data/dataset/jas_image.c-jas_image_encode.c.txt\n",
      "dirs: 28it [00:17,  4.35it/s]/home/concoction/data/dataset/jp2_enc.c-jp2_encode.c.txt\n",
      "dirs: 29it [00:19,  1.66it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_qcc_destroyparms.c.txt\n",
      "/home/concoction/data/dataset/jas_icc.c-jas_iccattrval_create0.c.txt\n",
      "dirs: 31it [00:19,  2.56it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_qcd_destroyparms.c.txt\n",
      "/home/concoction/data/dataset/jas_cm.c-jas_cmshapmatlut_set.c.txt\n",
      "dirs: 33it [00:19,  3.28it/s]/home/concoction/data/dataset/jpc_mqenc.c-jpc_mqenc_create.c.txt\n",
      "dirs: 34it [00:20,  3.47it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmprof_create.c.txt\n",
      "dirs: 35it [00:20,  3.98it/s]/home/concoction/data/dataset/jas_stream.c-jas_stream_read.c.txt\n",
      "dirs: 36it [00:20,  4.44it/s]/home/concoction/data/dataset/jpc_mqenc.c-jpc_mqenc_init.c.txt\n",
      "/home/concoction/data/dataset/jpc_bs.c-jpc_bitstream_alloc.c.txt\n",
      "dirs: 38it [00:20,  6.20it/s]/home/concoction/data/dataset/jpc_tagtree.c-jpc_tagtree_alloc.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-jas_stream_gobble.c.txt\n",
      "dirs: 40it [00:20,  6.94it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_qcd_putparms.c.txt\n",
      "/home/concoction/data/dataset/jp2_cod.c-jp2_colr_putdata.c.txt\n",
      "dirs: 42it [00:21,  3.56it/s]/home/concoction/data/dataset/jpc_tagtree.c-jpc_tagtree_setvalue.c.txt\n",
      "dirs: 43it [00:22,  3.73it/s]/home/concoction/data/dataset/jasper.c-cmdopts_destroy.c.txt\n",
      "/home/concoction/data/dataset/jpc_enc.c-cblk_destroy.c.txt\n",
      "dirs: 45it [00:23,  2.36it/s]/home/concoction/data/dataset/jas_seq.c-jas_matrix_destroy.c.txt\n",
      "dirs: 46it [00:23,  2.69it/s]/home/concoction/data/dataset/pnm_dec.c-pnm_getuintstr.c.txt\n",
      "dirs: 47it [00:23,  2.74it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccattrtab_lookup.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-jas_stream_create.c.txt\n",
      "dirs: 49it [00:24,  3.92it/s]/home/concoction/data/dataset/jpc_enc.c-rlvl_create.c.txt\n",
      "/home/concoction/data/dataset/jpc_enc.c-cblk_create.c.txt\n",
      "/home/concoction/data/dataset/jas_image.c-bitstoint.c.txt\n",
      "dirs: 52it [00:24,  5.60it/s]/home/concoction/data/dataset/jas_getopt.c-jas_optlookup.c.txt\n",
      "dirs: 53it [00:24,  5.66it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_qcx_destroycompparms.c.txt\n",
      "/home/concoction/data/dataset/jas_image.c-jas_image_cmpt_destroy.c.txt\n",
      "dirs: 55it [00:24,  6.84it/s]/home/concoction/data/dataset/jas_seq.c-jas_seq2d_create.c.txt\n",
      "dirs: 56it [00:24,  6.59it/s]/home/concoction/data/dataset/jpc_tsfb.c-jpc_tsfb_getbands2.c.txt\n",
      "/home/concoction/data/dataset/jpc_cs.c-jpc_putuint16.c.txt\n",
      "dirs: 58it [00:25,  6.99it/s]/home/concoction/data/dataset/jpc_enc.c-jpc_enc_destroy.c.txt\n",
      "dirs: 59it [00:25,  3.94it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_com_putparms.c.txt\n",
      "dirs: 60it [00:26,  4.44it/s]/home/concoction/data/dataset/jpc_t2enc.c-jpc_enc_encpkts.c.txt\n",
      "dirs: 61it [00:26,  4.99it/s]/home/concoction/data/dataset/jpc_tsfb.c-jpc_tsfb_destroy.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-jas_strtoopenmode.c.txt\n",
      "/home/concoction/data/dataset/pnm_dec.c-pnm_getsintstr.c.txt\n",
      "dirs: 64it [00:27,  3.04it/s]/home/concoction/data/dataset/jas_image.c-jas_image_cmpt_create.c.txt\n",
      "/home/concoction/data/dataset/jas_cm.c-jas_cmprof_destroy.c.txt\n",
      "dirs: 66it [00:27,  3.60it/s]/home/concoction/data/dataset/jpc_qmfb.c-jpc_ft_fwdlift_row.c.txt\n",
      "dirs: 67it [00:28,  2.47it/s]/home/concoction/data/dataset/jpc_t2enc.c-jpc_putcommacode.c.txt\n",
      "dirs: 68it [00:28,  2.90it/s]/home/concoction/data/dataset/jp2_cod.c-jp2_colr_destroy.c.txt\n",
      "/home/concoction/data/dataset/jas_icc.c-jas_iccattrvalinfo_lookup.c.txt\n",
      "dirs: 70it [00:29,  4.05it/s]/home/concoction/data/dataset/jpc_t1cod.c-jpc_getzcctxno.c.txt\n",
      "dirs: 71it [00:30,  2.22it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccgetxyz.c.txt\n",
      "dirs: 72it [00:30,  2.36it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccprof_getattr.c.txt\n",
      "dirs: 73it [00:30,  2.86it/s]/home/concoction/data/dataset/jas_stream.c-file_write.c.txt\n",
      "/home/concoction/data/dataset/jpc_t1cod.c-JPC_ISTERMINATED.c.txt\n",
      "dirs: 75it [00:30,  4.33it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmshapmatlut_cleanup.c.txt\n",
      "/home/concoction/data/dataset/jas_cm.c-jas_cmprof_createfromclrspc.c.txt\n",
      "dirs: 77it [00:31,  4.32it/s]/home/concoction/data/dataset/jp2_cod.c-jp2_box_put.c.txt\n",
      "dirs: 78it [00:35,  1.05it/s]/home/concoction/data/dataset/jpc_enc.c-tcmpt_create.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-file_read.c.txt\n",
      "/home/concoction/data/dataset/jas_stream.c-jas_stream_destroy.c.txt\n",
      "dirs: 81it [00:35,  1.83it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccattrtab_copy.c.txt\n",
      "dirs: 82it [00:35,  2.13it/s]/home/concoction/data/dataset/jas_seq.c-jas_matrix_bindsub.c.txt\n",
      "dirs: 83it [00:35,  2.20it/s]/home/concoction/data/dataset/jpc_enc.c-jpc_enc_tile_destroy.c.txt\n",
      "dirs: 84it [00:36,  2.33it/s]/home/concoction/data/dataset/jpc_tsfb.c-jpc_tsfb_analyze2.c.txt\n",
      "dirs: 85it [00:36,  2.32it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmpxformseq_create.c.txt\n",
      "dirs: 86it [00:36,  2.67it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccprof_createfromclrspc.c.txt\n",
      "dirs: 87it [00:36,  3.23it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccgetuint64.c.txt\n",
      "/home/concoction/data/dataset/jp2_enc.c-clrspctojp2.c.txt\n",
      "dirs: 89it [00:37,  4.33it/s]/home/concoction/data/dataset/jas_image.c-jas_image_create.c.txt\n",
      "dirs: 90it [00:37,  4.39it/s]/home/concoction/data/dataset/jas_stream.c-mem_read.c.txt\n",
      "/home/concoction/data/dataset/jas_string.c-jas_strdup.c.txt\n",
      "dirs: 92it [00:37,  6.15it/s]/home/concoction/data/dataset/jas_debug.c-jas_getdbglevel.c.txt\n",
      "/home/concoction/data/dataset/jas_cm.c-jas_cmpxformseq_insertpxform.c.txt\n",
      "dirs: 94it [00:38,  3.00it/s]/home/concoction/data/dataset/jpc_enc.c-jpc_enc_encodetiledata.c.txt\n",
      "/home/concoction/data/dataset/jas_tvp.c-jas_tvparser_next.c.txt\n",
      "dirs: 96it [00:40,  1.88it/s]/home/concoction/data/dataset/jas_init.c-jas_init.c.txt\n",
      "/home/concoction/data/dataset/jpc_t1cod.c-jpc_getscctxno.c.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirs: 98it [00:41,  2.21it/s]/home/concoction/data/dataset/jas_cm.c-jas_clrspc_numchans.c.txt\n",
      "dirs: 99it [00:41,  2.56it/s]/home/concoction/data/dataset/jpc_t2cod.c-jpc_pi_create0.c.txt\n",
      "dirs: 100it [00:41,  2.99it/s]/home/concoction/data/dataset/jas_stream.c-mem_close.c.txt\n",
      "dirs: 101it [00:41,  3.33it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmshapmat_invmat.c.txt\n",
      "/home/concoction/data/dataset/jas_seq.c-jas_matrix_create.c.txt\n",
      "dirs: 103it [00:42,  3.30it/s]/home/concoction/data/dataset/pnm_cod.c-pnm_fmt.c.txt\n",
      "dirs: 104it [00:42,  3.78it/s]/home/concoction/data/dataset/jpc_mqenc.c-jpc_mqenc_setctxs.c.txt\n",
      "/home/concoction/data/dataset/jpc_enc.c-pass_destroy.c.txt\n",
      "/home/concoction/data/dataset/jpc_t2cod.c-jpc_pi_init.c.txt\n",
      "dirs: 107it [00:42,  6.33it/s]/home/concoction/data/dataset/jas_stream.c-mem_seek.c.txt\n",
      "/home/concoction/data/dataset/jpc_tagtree.c-jpc_tagtree_reset.c.txt\n",
      "dirs: 109it [00:42,  5.87it/s]/home/concoction/data/dataset/jpc_t1enc.c-jpc_encclnpass.c.txt\n",
      "dirs: 110it [00:44,  1.93it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_ms_create.c.txt\n",
      "/home/concoction/data/dataset/jas_tvp.c-jas_tvparser_destroy.c.txt\n",
      "dirs: 112it [00:45,  2.70it/s]/home/concoction/data/dataset/jas_cm.c-jas_cmpxformseq_delete.c.txt\n",
      "dirs: 113it [00:45,  3.02it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_ms_destroy.c.txt\n",
      "dirs: 114it [00:45,  3.39it/s]/home/concoction/data/dataset/jpc_tsfb.c-jpc_tsfb_analyze.c.txt\n",
      "/home/concoction/data/dataset/jas_seq.c-jas_seq2d_bindsub.c.txt\n",
      "dirs: 116it [00:45,  4.84it/s]/home/concoction/data/dataset/jpc_cs.c-jpc_putms.c.txt\n",
      "dirs: 117it [00:46,  2.37it/s]/home/concoction/data/dataset/jas_debug.c-jas_setdbglevel.c.txt\n",
      "/home/concoction/data/dataset/jp2_cod.c-jp2_putuint8.c.txt\n",
      "dirs: 119it [00:46,  3.50it/s]/home/concoction/data/dataset/jpc_t1enc.c-jpc_enc_enccblk.c.txt\n",
      "dirs: 120it [00:47,  3.63it/s]/home/concoction/data/dataset/jp2_cod.c-jp2_boxinfolookup.c.txt\n",
      "/home/concoction/data/dataset/jas_icc.c-jas_iccprof_create.c.txt\n",
      "dirs: 122it [00:47,  4.36it/s]/home/concoction/data/dataset/jas_stream.c-jas_stream_close.c.txt\n",
      "/home/concoction/data/dataset/jpc_t1cod.c-jpc_getspb.c.txt\n",
      "dirs: 124it [00:47,  4.56it/s]/home/concoction/data/dataset/jpc_t1cod.c-jpc_pow2i.c.txt\n",
      "/home/concoction/data/dataset/jpc_cs.c-jpc_qcc_putparms.c.txt\n",
      "dirs: 126it [00:48,  5.28it/s]/home/concoction/data/dataset/jas_icc.c-jas_iccprof_readhdr.c.txt\n",
      "^C\n",
      "dirs: 126it [14:07,  6.73s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"preprocess.py\", line 196, in <module>\n",
      "    preprocess(fpath,storedDir)\n",
      "  File \"preprocess.py\", line 151, in preprocess\n",
      "    **X_path_embedding, output_hidden_states=True, return_dict=True\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 968, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 566, in forward\n",
      "    output_attentions,\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 460, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 393, in forward\n",
      "    output_attentions,\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 252, in forward\n",
      "    mixed_query_layer = self.query(hidden_states)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "KeyboardInterrupt\n",
      "Execution path representation stored in /home/concoction/data/feature_path\n"
     ]
    }
   ],
   "source": [
    "# Execution path representation (shown as Sec. 3.6.1)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/dataset')\n",
    "storedDir=os.path.join(current,'concoction/data/feature_path')\n",
    "dir=os.path.join(current,'concoction/pathSelection')\n",
    "if not os.path.exists(storedDir):\n",
    "    os.mkdir(storedDir)\n",
    "! cd $dir && python preprocess.py --data_path $path --stored_path $storedDir\n",
    "print(f\"Execution path representation stored in {storedDir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n",
      "Load Data....\n",
      "100%|█████████████████████████████████████████| 126/126 [00:16<00:00,  7.64it/s]\n",
      "saved the select path to /home/concoction/data/feature_path_text\n"
     ]
    }
   ],
   "source": [
    "# Active learning for path selection (Sec. 3.6.2)\n",
    "#data preprocess\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'concoction/data/feature')\n",
    "storedDir=os.path.join(current,'concoction/data/feature_path')\n",
    "final_storedDir=os.path.join(current,'concoction/data/feature_path_text')\n",
    "dir=os.path.join(current,'concoction/pathSelection')\n",
    "if not os.path.exists(storedDir):\n",
    "    os.mkdir(storedDir)\n",
    "# ! cd $dir && python preprocess.py --data_path $path --stored_path $storedDir\n",
    "#select path\n",
    "!cd $dir && python train.py --data_path $storedDir --stored_path $final_storedDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored in the /homee/concoction/data/feature_result\r\n"
     ]
    }
   ],
   "source": [
    "# Symbolic execution for chosen paths (Sec. 3.6.3)\n",
    "import os\n",
    "current_folder = os.path.abspath('.')\n",
    "current=current_folder\n",
    "path=os.path.join(current,'feature/dynamic/map.py')\n",
    "dataset=os.path.join(current,'concoction/data/dataset')\n",
    "feature_path_text=os.path.join(current,'concoction/data/feature_path_text')\n",
    "final_path=os.path.join(current,'concoction/data/feature_result')\n",
    "!python $path $feature_path_text $dataset $final_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fuzzing for Test Case Generation*:\n",
    "\n",
    " We use fuzzing techniques to generate test cases for functions predicted to contain potential vulnerabilities, aiming to automate the testing process and minimize the need for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.727404100Z",
     "start_time": "2023-05-08T13:43:09.711400500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilizing AFL++ To Objective project (Shown as Sec. 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Experimental Evaluation\n",
    "\n",
    "Here, we provide a small-sized evaluation to showcase the working mechanism of Concoction bug detection. A full-scale evaluation, which takes more than a day to run, is provided through the Docker image (with detailed instructions on our project Github).\n",
    "\n",
    "### Large-scale Testing (Section 5.1)\n",
    "\n",
    " This part (add a web link) gives a quantified summary of Concoction for detecting function-level code vulnerabilities across the 20 projects listed in Table 1 in our papaer.\n",
    "\n",
    "This demo corresponds to Table 5 of the submitted manuscript.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Open Dataset (Section  5.2)\n",
    "\n",
    "We now evaluate our vulnerability detection model on the SARD and CVE datasets in Table 2 in paper.\n",
    "\n",
    "This demo corresponds to Figure 9 and 10 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*\n",
    "\n",
    "#### *Sard Dataset*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.740861300Z",
     "start_time": "2023-05-08T13:43:09.725404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vuldeepecker1/bin/python vuldeepecker.py --data_path /home/ExperimentalEvaluation/data/sard/cwe416 --mode train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/vuldeepecker1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2023-12-05 22:01:16.289410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-12-05 22:01:16.372572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:19:00.0\n",
      "2023-12-05 22:01:16.372975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:1b:00.0\n",
      "2023-12-05 22:01:16.373305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:68:00.0\n",
      "2023-12-05 22:01:16.373549: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.373688: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.373812: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.373928: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.374049: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.374168: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.374287: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:01:16.374300: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-12-05 22:01:16.374644: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2023-12-05 22:01:16.505360: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz\n",
      "2023-12-05 22:01:16.519038: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x92a9370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-05 22:01:16.519097: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-05 22:01:17.089459: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa987da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-05 22:01:17.089529: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:01:17.089551: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:01:17.089568: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:01:17.092856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-12-05 22:01:17.092903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "WARNING:tensorflow:From /root/anaconda3/envs/vuldeepecker1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============random_state===========\n",
      " 42\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn\n",
      "tracking <tf.Variable 'Variable_4:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_5:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_6:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_7:0' shape=() dtype=int32> fn\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 600)               842400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,113,602\n",
      "Trainable params: 1,113,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 156 samples, validate on 39 samples\n",
      "Epoch 1/2\n",
      "\n",
      " 64/156 [===========>..................] - ETA: 1s - loss: 0.7031 - precision: 0.5128 - recall: 0.6250 - f1_score: 0.5634 - accuracy: 0.5156\n",
      "128/156 [=======================>......] - ETA: 0s - loss: 0.6464 - precision: 0.5689 - recall: 0.5699 - f1_score: 0.5639 - accuracy: 0.5781\n",
      "156/156 [==============================] - 2s 10ms/step - loss: 0.6181 - precision: 0.6000 - recall: 0.5791 - f1_score: 0.5854 - accuracy: 0.6282 - val_loss: 0.3829 - val_precision: 0.8333 - val_recall: 1.0000 - val_f1_score: 0.9091 - val_accuracy: 0.9231\n",
      "Epoch 2/2\n",
      "\n",
      " 64/156 [===========>..................] - ETA: 0s - loss: 0.4116 - precision: 0.8864 - recall: 1.0000 - f1_score: 0.9398 - accuracy: 0.9219\n",
      "128/156 [=======================>......] - ETA: 0s - loss: 0.3215 - precision: 0.9047 - recall: 1.0000 - f1_score: 0.9499 - accuracy: 0.9531\n",
      "156/156 [==============================] - 0s 3ms/step - loss: 0.3023 - precision: 0.9035 - recall: 1.0000 - f1_score: 0.9492 - accuracy: 0.9423 - val_loss: 0.0864 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1_score: 1.0000 - val_accuracy: 1.0000\n",
      "[2023-12-05 22:01:20] \u001b[32mFinal result: 1.0\u001b[0m\n",
      "saving the trained model in 1.0_42.h5\n",
      "Accuracy is... 1.0\n",
      "True recall is... 1.0\n",
      "Precision is... 1.0\n",
      "F1 score is... 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset and preprocess (use parameter to change CWE type and method,\n",
    "#                                     like FUN A( dataset = 'CWE-123', method = 'Vuldeepecker' ))\n",
    "from demo2.demo2 import *\n",
    "# Train and test.\n",
    "EvaluationSard(\"CWE-416\",\"vuldeepecker\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Github Dataset*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.758882900Z",
     "start_time": "2023-05-08T13:43:09.741861500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vuldeepecker1/bin/python vuldeepecker.py --data_path /home/ExperimentalEvaluation/data/github_0.65 --mode train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9950 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/vuldeepecker1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2023-12-05 22:03:11.931644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-12-05 22:03:11.998739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:19:00.0\n",
      "2023-12-05 22:03:11.999050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:1b:00.0\n",
      "2023-12-05 22:03:11.999320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: NVIDIA GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\n",
      "pciBusID: 0000:68:00.0\n",
      "2023-12-05 22:03:11.999466: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999546: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999615: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999684: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999751: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999821: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999888: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:/home/Infer/libpng-1.2.7/.libs:\n",
      "2023-12-05 22:03:11.999897: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-12-05 22:03:12.000263: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2023-12-05 22:03:12.005168: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299990000 Hz\n",
      "2023-12-05 22:03:12.006792: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa2fccb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-05 22:03:12.006852: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-05 22:03:12.602046: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb9ddfd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-05 22:03:12.602118: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:03:12.602144: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:03:12.602163: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-12-05 22:03:12.602584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-12-05 22:03:12.602616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n",
      "WARNING:tensorflow:From /root/anaconda3/envs/vuldeepecker1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============random_state===========\n",
      " 42\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn\n",
      "tracking <tf.Variable 'Variable_4:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_5:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_6:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_7:0' shape=() dtype=int32> fn\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 600)               842400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,113,602\n",
      "Trainable params: 1,113,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1307 samples, validate on 327 samples\n",
      "Epoch 1/2\n",
      "\n",
      "  64/1307 [>.............................] - ETA: 12s - loss: 0.7129 - precision: 0.2609 - recall: 0.2400 - f1_score: 0.2500 - accuracy: 0.4375\n",
      " 128/1307 [=>............................] - ETA: 7s - loss: 0.7834 - precision: 0.2609 - recall: 0.1684 - f1_score: 0.1956 - accuracy: 0.4297 \n",
      " 192/1307 [===>..........................] - ETA: 5s - loss: 0.7407 - precision: 0.3438 - recall: 0.2035 - f1_score: 0.2491 - accuracy: 0.5104\n",
      " 256/1307 [====>.........................] - ETA: 4s - loss: 0.7278 - precision: 0.3817 - recall: 0.2586 - f1_score: 0.3011 - accuracy: 0.5078\n",
      " 320/1307 [======>.......................] - ETA: 3s - loss: 0.7309 - precision: 0.4054 - recall: 0.3145 - f1_score: 0.3445 - accuracy: 0.5063\n",
      " 384/1307 [=======>......................] - ETA: 3s - loss: 0.7215 - precision: 0.4219 - recall: 0.3612 - f1_score: 0.3781 - accuracy: 0.5104\n",
      " 448/1307 [=========>....................] - ETA: 3s - loss: 0.7151 - precision: 0.4328 - recall: 0.3952 - f1_score: 0.4018 - accuracy: 0.5134\n",
      " 512/1307 [==========>...................] - ETA: 2s - loss: 0.7169 - precision: 0.4423 - recall: 0.4169 - f1_score: 0.4187 - accuracy: 0.5117\n",
      " 576/1307 [============>.................] - ETA: 2s - loss: 0.7063 - precision: 0.4509 - recall: 0.4310 - f1_score: 0.4313 - accuracy: 0.5295\n",
      " 640/1307 [=============>................] - ETA: 2s - loss: 0.7011 - precision: 0.4593 - recall: 0.4417 - f1_score: 0.4418 - accuracy: 0.5437\n",
      " 704/1307 [===============>..............] - ETA: 1s - loss: 0.6961 - precision: 0.4663 - recall: 0.4487 - f1_score: 0.4496 - accuracy: 0.5469\n",
      " 768/1307 [================>.............] - ETA: 1s - loss: 0.6918 - precision: 0.4726 - recall: 0.4537 - f1_score: 0.4558 - accuracy: 0.5534\n",
      " 832/1307 [==================>...........] - ETA: 1s - loss: 0.6853 - precision: 0.4787 - recall: 0.4577 - f1_score: 0.4614 - accuracy: 0.5601\n",
      " 896/1307 [===================>..........] - ETA: 1s - loss: 0.6772 - precision: 0.4846 - recall: 0.4620 - f1_score: 0.4669 - accuracy: 0.5748\n",
      " 960/1307 [=====================>........] - ETA: 1s - loss: 0.6713 - precision: 0.4903 - recall: 0.4654 - f1_score: 0.4718 - accuracy: 0.5802\n",
      "1024/1307 [======================>.......] - ETA: 0s - loss: 0.6666 - precision: 0.4959 - recall: 0.4688 - f1_score: 0.4765 - accuracy: 0.5869\n",
      "1088/1307 [=======================>......] - ETA: 0s - loss: 0.6600 - precision: 0.5018 - recall: 0.4727 - f1_score: 0.4817 - accuracy: 0.5956\n",
      "1152/1307 [=========================>....] - ETA: 0s - loss: 0.6548 - precision: 0.5070 - recall: 0.4771 - f1_score: 0.4868 - accuracy: 0.5998\n",
      "1216/1307 [==========================>...] - ETA: 0s - loss: 0.6500 - precision: 0.5118 - recall: 0.4820 - f1_score: 0.4919 - accuracy: 0.6061\n",
      "1280/1307 [============================>.] - ETA: 0s - loss: 0.6431 - precision: 0.5167 - recall: 0.4870 - f1_score: 0.4971 - accuracy: 0.6156\n",
      "1307/1307 [==============================] - 4s 3ms/step - loss: 0.6400 - precision: 0.5212 - recall: 0.4916 - f1_score: 0.5018 - accuracy: 0.6182 - val_loss: 0.4534 - val_precision: 0.9786 - val_recall: 0.4934 - val_f1_score: 0.6550 - val_accuracy: 0.7615\n",
      "Epoch 2/2\n",
      "\n",
      "  64/1307 [>.............................] - ETA: 2s - loss: 0.5146 - precision: 1.0000 - recall: 0.5143 - f1_score: 0.6792 - accuracy: 0.7344\n",
      " 128/1307 [=>............................] - ETA: 2s - loss: 0.4591 - precision: 0.9750 - recall: 0.5540 - f1_score: 0.7050 - accuracy: 0.7812\n",
      " 192/1307 [===>..........................] - ETA: 2s - loss: 0.4707 - precision: 0.9672 - recall: 0.5660 - f1_score: 0.7128 - accuracy: 0.7708\n",
      " 256/1307 [====>.........................] - ETA: 2s - loss: 0.4582 - precision: 0.9494 - recall: 0.5886 - f1_score: 0.7240 - accuracy: 0.7852\n",
      " 320/1307 [======>.......................] - ETA: 2s - loss: 0.4639 - precision: 0.9250 - recall: 0.6138 - f1_score: 0.7326 - accuracy: 0.7812\n",
      " 384/1307 [=======>......................] - ETA: 2s - loss: 0.4729 - precision: 0.9060 - recall: 0.6347 - f1_score: 0.7394 - accuracy: 0.7839\n",
      " 448/1307 [=========>....................] - ETA: 2s - loss: 0.4614 - precision: 0.8958 - recall: 0.6485 - f1_score: 0.7451 - accuracy: 0.7902\n",
      " 512/1307 [==========>...................] - ETA: 1s - loss: 0.4429 - precision: 0.8897 - recall: 0.6596 - f1_score: 0.7505 - accuracy: 0.8027\n",
      " 576/1307 [============>.................] - ETA: 1s - loss: 0.4312 - precision: 0.8860 - recall: 0.6685 - f1_score: 0.7554 - accuracy: 0.8108\n",
      " 640/1307 [=============>................] - ETA: 1s - loss: 0.4178 - precision: 0.8837 - recall: 0.6755 - f1_score: 0.7594 - accuracy: 0.8188\n",
      " 704/1307 [===============>..............] - ETA: 1s - loss: 0.4132 - precision: 0.8814 - recall: 0.6816 - f1_score: 0.7628 - accuracy: 0.8239\n",
      " 768/1307 [================>.............] - ETA: 1s - loss: 0.4024 - precision: 0.8792 - recall: 0.6882 - f1_score: 0.7663 - accuracy: 0.8281\n",
      " 832/1307 [==================>...........] - ETA: 1s - loss: 0.3975 - precision: 0.8777 - recall: 0.6937 - f1_score: 0.7694 - accuracy: 0.8305\n",
      " 896/1307 [===================>..........] - ETA: 0s - loss: 0.3941 - precision: 0.8766 - recall: 0.6991 - f1_score: 0.7725 - accuracy: 0.8337\n",
      " 960/1307 [=====================>........] - ETA: 0s - loss: 0.3903 - precision: 0.8760 - recall: 0.7038 - f1_score: 0.7754 - accuracy: 0.8333\n",
      "1024/1307 [======================>.......] - ETA: 0s - loss: 0.3879 - precision: 0.8751 - recall: 0.7087 - f1_score: 0.7782 - accuracy: 0.8369\n",
      "1088/1307 [=======================>......] - ETA: 0s - loss: 0.3796 - precision: 0.8747 - recall: 0.7136 - f1_score: 0.7811 - accuracy: 0.8438\n",
      "1152/1307 [=========================>....] - ETA: 0s - loss: 0.3760 - precision: 0.8746 - recall: 0.7184 - f1_score: 0.7841 - accuracy: 0.8481\n",
      "1216/1307 [==========================>...] - ETA: 0s - loss: 0.3789 - precision: 0.8745 - recall: 0.7223 - f1_score: 0.7866 - accuracy: 0.8446\n",
      "1280/1307 [============================>.] - ETA: 0s - loss: 0.3704 - precision: 0.8748 - recall: 0.7259 - f1_score: 0.7890 - accuracy: 0.8492\n",
      "1307/1307 [==============================] - 3s 3ms/step - loss: 0.3664 - precision: 0.8752 - recall: 0.7295 - f1_score: 0.7915 - accuracy: 0.8523 - val_loss: 0.2505 - val_precision: 0.9420 - val_recall: 0.9669 - val_f1_score: 0.9542 - val_accuracy: 0.9541\n",
      "[2023-12-05 22:03:21] \u001b[32mFinal result: 0.9552941176470588\u001b[0m\n",
      "saving the trained model in 0.9552941176470588_42.h5\n",
      "Accuracy is... 0.9535452322738386\n",
      "True recall is... 0.9485981308411215\n",
      "Precision is... 0.9620853080568721\n",
      "F1 score is... 0.9552941176470588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from demo2.demo2 import *\n",
    "EvaluationGithub(\"Github\",\"vuldeepecker\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 9 and 10 (Section 5.2) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.772886Z",
     "start_time": "2023-05-08T13:43:09.757882600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 9: Evaluation on standard vulnerability databases. Min-max bars show performance across vulnerability types.\n",
    "# Output Figure 10:  Evaluation on the CVE dataset Concoction gives the best performance across evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3:  Evaluation on Opensource Projects (Section 5.3)\n",
    "\n",
    "We now compare to the baseline methods by applying them to the three open-source projects in Table 3 with a total of 35 CVEs reported by independent users.\n",
    "\n",
    "This demo corresponds to Figure 11 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = xx minutes for one benchmark*\n",
    "\n",
    "### Client RL Deployment Demo\n",
    "\n",
    "This demo shows how to apply the saved client RL to optimize a test program for Code Size Reduction. \n",
    "\n",
    "*approximate runtime ~ 15 minutes*\n",
    "\n",
    "#### Performance evaluation on benchmarks\n",
    "Benchmarks: Sqlite, Libtiff, Libpng.\n",
    "*approximate runtime ~ 20 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.790890100Z",
     "start_time": "2023-05-08T13:43:09.773886200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and test benchmarks (Input benchmark and method like FUN A( project = 'Sqlite', method = 'Concoction')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-scale evaluation data\n",
    "We now generate the table using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 11 (Section 5.3) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.804893200Z",
     "start_time": "2023-05-08T13:43:09.788889500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 11: The number of vulnerabilities identified Concoction and other methods for open-source projects in Table 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis (Alternative)\n",
    "\n",
    "####  DL model implementation choices.\n",
    "\n",
    "we evaluate several variants of Concoction on the CVE dataset.\n",
    "\n",
    "This demo corresponds to Figure 12 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = xx minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.822897400Z",
     "start_time": "2023-05-08T13:43:09.805893300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and test CVE benchmarks (Input method like FUN A( method = 'Static')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-scale evaluation data\n",
    "We now generate the figure using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 12 (Section 5.4) of the submitted manuscript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.835900200Z",
     "start_time": "2023-05-08T13:43:09.819896400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 12:  Comparing implementation variants of Concoction on the CVE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
