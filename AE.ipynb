{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Evaluation Instructions: #506   Combining Static and Dynamic Code Information for Software Vulnerability Prediction\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This interactive Jupyter notebook provides a small-scale demo to showcase the program representation, vulnerability detection, and prediction of vulnerability detection discussed in the paper.\n",
    "\n",
    "The main results of our CCS 2023 paper involve comparing the performance of our vulnerability detection with prior machine learning-based approaches. The evaluation presented in our paper was conducted on a much larger dataset and for a longer duration. The intention of this notebook is to provide minimal working examples that can be evaluated within a reasonable time frame.\n",
    "\n",
    "## Instructions for Experimental Workflow:\n",
    "\n",
    "Before you start, please first make a copy of the notebook by going to the landing page. Then select the checkbox next to the notebook titled *main.ipynb*, then click \"**Duplicate**\".\n",
    "\n",
    "Click the name of the newly created Jupyter Notebook, e.g. **AE-Copy1.ipynb**. Next, select \"**Kernel**\" > \"**Restart & Clear Output**\". Then, repeatedly press the play button (the tooltip is \"run cell, select below\") to step through each cell of the notebook.\n",
    "\n",
    "Alternatively, select each cell in turn and use \"**Cell**\"> \"**Run Cell**\" from the menu to run specific cells. Note that some cells depend on previous cells being executed. If any errors occur, ensure all previous cells have been executed.\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "**Some cells can take more than half an hour to complete; please wait for the results until step to the next cell.** \n",
    "\n",
    "High load can lead to a long wait for results. This may occur if multiple reviewers are simultaneously trying to generate results. \n",
    "\n",
    "The experiments are customisable as the code provided in the Jupyter Notebook can be edited on the spot. Simply type your changes into the code blocks and re-run using **Cell > Run Cells** from the menu.\n",
    "\n",
    "## Links to The Paper\n",
    "\n",
    "For each step, we note the section number of the submitted version where the relevant technique is described or data is presented.\n",
    "\n",
    "The main results are presented in Figures 9-12 of the submitted paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: The Concoction Model Architecture\n",
    "\n",
    "This demo corresponds to the architecture given in Figure 3. Note that This is a small-scale demo for vulnerability detection. The full-scale evaluation used in the paper takes over 24 hours to run.\n",
    "\n",
    "## Step 1. Program representation\n",
    "\n",
    "The program representation component maps the input source code and dynamic symbolic execution traces of the target function into a numerical embedding vector.\n",
    "\n",
    "#### *Static representation model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.666390500Z",
     "start_time": "2023-05-08T13:43:09.658388600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION/getFeature/data/file\n",
      "/home/CONCOCTION/getFeature/staticFeature\n",
      "finished:0.00%   Batch 1\n",
      "Get function segmentation ! \n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.languagespecific.fuzzyc.TypeDeclStubCreator\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.languagespecific.fuzzyc.TypeDeclStubCreator, after 20ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.languagespecific.fuzzyc.MethodStubCreator\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.languagespecific.fuzzyc.MethodStubCreator, after 44ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.methoddecorations.MethodDecoratorPass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.methoddecorations.MethodDecoratorPass, after 7ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.linking.capturinglinker.CapturingLinker\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.linking.capturinglinker.CapturingLinker, after 3ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.linking.linker.Linker\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.linking.linker.Linker, after 27ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.compat.bindingtablecompat.BindingTableCompat\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.compat.bindingtablecompat.BindingTableCompat, after 13ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.BindingMethodOverridesPass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.BindingMethodOverridesPass, after 11ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.linking.calllinker.CallLinker\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.linking.calllinker.CallLinker, after 4ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.linking.memberaccesslinker.MemberAccessLinker\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.linking.memberaccesslinker.MemberAccessLinker, after 13ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.methodexternaldecorator.MethodExternalDecoratorPass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.methodexternaldecorator.MethodExternalDecoratorPass, after 1ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.containsedges.ContainsEdgePass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.containsedges.ContainsEdgePass, after 20ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.namespacecreator.NamespaceCreator\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.namespacecreator.NamespaceCreator, after 5ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.cfgdominator.CfgDominatorPass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.cfgdominator.CfgDominatorPass, after 25ms\n",
      "Start of enhancement: io.shiftleft.semanticcpg.passes.codepencegraph.CdgPass\n",
      "End of enhancement: io.shiftleft.semanticcpg.passes.codepencegraph.CdgPass, after 13ms\n",
      "Start of enhancement: io.shiftleft.dataflowengine.passes.propagateedges.PropagateEdgePass\n",
      "End of enhancement: io.shiftleft.dataflowengine.passes.propagateedges.PropagateEdgePass, after 5ms\n",
      "Start of enhancement: io.shiftleft.dataflowengine.passes.reachingdef.ReachingDefPass\n",
      "End of enhancement: io.shiftleft.dataflowengine.passes.reachingdef.ReachingDefPass, after 25ms\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/interpBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/replBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/sourceBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/frontEndBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/DefaultPredef.sc\u001b[39m\n",
      "\u001b[34mCompiling /home/CONCOCTION/getFeature/staticFeature/joern-cli/(console)\u001b[39m\n",
      "\n",
      "     ??? ??????? ??????????????? ????   ???\n",
      "     ?????????????????????????????????  ???\n",
      "     ??????   ?????????  ?????????????? ???\n",
      "??   ??????   ?????????  ??????????????????\n",
      "????????????????????????????  ?????? ??????\n",
      " ??????  ??????? ???????????  ??????  ?????\n",
      "      \n",
      "Welcome to ShiftLeft Ocular/Joern\n",
      "\u001b[1A\u001b[9999D\u001b[0J\u001b[35mjoern> \u001b[39m\n",
      "loadCpg(\u001b[32m\"/home/CONCOCTION/getFeature/staticFeature/joern-cli/parse_result/good.bin\"\u001b[39m) \u001b[1A\u001b[9999D\u001b[1B\u001b[4C\n",
      "\u001b[36mres0\u001b[39m: \u001b[32mOption\u001b[39m[\u001b[32mCpg\u001b[39m] = \u001b[33mSome\u001b[39m(io.shiftleft.codepropertygraph.Cpg@6bc72ab6)\n",
      "\n",
      "\u001b[1A\u001b[9999D\u001b[0J\u001b[35mjoern> \u001b[39m\n",
      "cpg.runScript(\u001b[32m\"/home/CONCOCTION/getFeature/staticFeature/joern-cli/graph/allgood.sc\"\u001b[39m) \u001b[1A\u001b[9999D\u001b[1B\u001b[5C\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/interpBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/replBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/sourceBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/frontEndBridge.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/DefaultPredef.sc\u001b[39m\n",
      "\u001b[34mCompiling (synthetic)/ammonite/predef/ArgsPredef.sc\u001b[39m\n",
      "\u001b[34mCompiling /home/CONCOCTION/getFeature/staticFeature/joern-cli/(console)\u001b[39m\n",
      "\u001b[34mCompiling /home/CONCOCTION/getFeature/staticFeature/joern-cli/graph/allgood.sc\u001b[39m\n",
      "16\n",
      "\u001b[36mres1\u001b[39m: \u001b[32mAny\u001b[39m = ()\n",
      "\n",
      "\u001b[9999D\u001b[0J\u001b[35mjoern> \u001b[39mexit \u001b[9999D\u001b[4C\u001b[7C\n",
      "\u001b[34mBye!\u001b[39m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ooooooooooooooover\n",
      "finished:0.00%   Batch 1\n",
      "416\n",
      "416\n",
      "Get seven edges!\n",
      "416.c-helperBad.c\n",
      "/home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperBad.c\n",
      "Parsing code at: /home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperBad.c - language: `NEWC`\n",
      "[+] Running language frontend\n",
      "=======================================================================================================\n",
      "Invoking CPG generator in a separate process. Note that the new process will consume additional memory.\n",
      "If you are importing a large codebase (and/or running into memory issues), please try the following:\n",
      "1) exit joern\n",
      "2) invoke the frontend: /home/feature/static/github/joern-cli_new/c2cpg.sh -J-Xmx16004m /home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperBad.c --output 1cpgbin.bin\n",
      "3) start joern, import the cpg: `importCpg(\"path/to/cpg\")`\n",
      "=======================================================================================================\n",
      "\n",
      "[+] Applying default overlays\n",
      "Successfully wrote graph to: /home/feature/static/github/joern-cli_new/1cpgbin.bin\n",
      "To load the graph, type `joern /home/feature/static/github/joern-cli_new/1cpgbin.bin`\n",
      "\u001b[34mCompiling /home/feature/static/github/joern-cli_new/(console)\u001b[39m\n",
      "416.c-helperGood.c\n",
      "/home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperGood.c\n",
      "Parsing code at: /home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperGood.c - language: `NEWC`\n",
      "[+] Running language frontend\n",
      "=======================================================================================================\n",
      "Invoking CPG generator in a separate process. Note that the new process will consume additional memory.\n",
      "If you are importing a large codebase (and/or running into memory issues), please try the following:\n",
      "1) exit joern\n",
      "2) invoke the frontend: /home/feature/static/github/joern-cli_new/c2cpg.sh -J-Xmx16004m /home/CONCOCTION/getFeature/staticFeature/cut/good/416.c-helperGood.c --output 1cpgbin.bin\n",
      "3) start joern, import the cpg: `importCpg(\"path/to/cpg\")`\n",
      "=======================================================================================================\n",
      "\n",
      "[+] Applying default overlays\n",
      "Successfully wrote graph to: /home/feature/static/github/joern-cli_new/1cpgbin.bin\n",
      "To load the graph, type `joern /home/feature/static/github/joern-cli_new/1cpgbin.bin`\n",
      "\u001b[34mCompiling /home/feature/static/github/joern-cli_new/(console)\u001b[39m\n",
      "Get cfg !\n",
      "Exception in thread \"main\" java.lang.NullPointerException\n",
      "\tat java.base/java.util.Arrays.sort(Arrays.java:1249)\n",
      "\tat cfg2path.GetCfgInfo.main(GetCfgInfo.java:208)\n",
      "Exception in thread \"main\" java.lang.NullPointerException\n",
      "\tat java.base/java.util.Arrays.sort(Arrays.java:1249)\n",
      "\tat cfg2path.GetCfgInfo.main(GetCfgInfo.java:208)\n",
      "Cfgresult completed\n",
      "/home/CONCOCTION/getFeature/staticFeature/static/416.c-helperGood.c.txt\n",
      "/home/CONCOCTION/getFeature/staticFeature/static/416.c-helperBad.c.txt\n",
      "Get static feature!\n"
     ]
    }
   ],
   "source": [
    "#  Extract Program Information (To show static code information like sec 3.3.1.)\n",
    "import os\n",
    "root=os.getcwd()\n",
    "projectPath=os.path.join(os.getcwd(),'getFeature/data/file')\n",
    "os.chdir(os.path.join(os.getcwd(),'getFeature/staticFeature'))\n",
    "!python GetStatic.py $projectPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION\n",
      "dirs: 1it [00:00, 24.76it/s]\n",
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "09/12/2023 06:18:12 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "09/12/2023 06:18:12 - INFO - __main__ - Training/evaluation parameters TrainingArguments(output_dir=/home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Sep12_06-18-12_5ab788861ade, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\n",
      "[INFO|configuration_utils.py:443] 2023-09-12 06:18:12,610 >> loading configuration file graphcodebert-base/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-12 06:18:12,611 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:443] 2023-09-12 06:18:12,612 >> loading configuration file graphcodebert-base/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-12 06:18:12,612 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2023-09-12 06:18:12,612 >> Model name 'graphcodebert-base' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'graphcodebert-base' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1718] 2023-09-12 06:18:12,612 >> Didn't find file graphcodebert-base/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1718] 2023-09-12 06:18:12,612 >> Didn't find file graphcodebert-base/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file graphcodebert-base/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file graphcodebert-base/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file graphcodebert-base/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1764] 2023-09-12 06:18:12,613 >> loading file graphcodebert-base/tokenizer_config.json\n",
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:1025] 2023-09-12 06:18:12,752 >> loading weights file graphcodebert-base/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1143] 2023-09-12 06:18:16,058 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2023-09-12 06:18:16,058 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at graphcodebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "[INFO|language_modeling.py:134] 2023-09-12 06:18:16,059 >> Creating features from dataset file at /home/CONCOCTION/model/data/output_static.txt\n",
      "[INFO|trainer.py:791] 2023-09-12 06:18:16,076 >> ***** Running training *****\n",
      "[INFO|trainer.py:792] 2023-09-12 06:18:16,077 >>   Num examples = 3\n",
      "[INFO|trainer.py:793] 2023-09-12 06:18:16,077 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:794] 2023-09-12 06:18:16,077 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:795] 2023-09-12 06:18:16,077 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:796] 2023-09-12 06:18:16,077 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:797] 2023-09-12 06:18:16,077 >>   Total optimization steps = 3\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:11<00:00,  3.81s/it][INFO|trainer.py:953] 2023-09-12 06:18:27,543 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 11.4662, 'train_samples_per_second': 0.262, 'epoch': 3.0}     \n",
      "100%|█████████████████████████████████████████████| 3/3 [00:11<00:00,  3.82s/it]\n",
      "[INFO|trainer.py:1344] 2023-09-12 06:18:27,544 >> Saving model checkpoint to /home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel\n",
      "[INFO|configuration_utils.py:300] 2023-09-12 06:18:27,546 >> Configuration saved in /home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel/config.json\n",
      "[INFO|modeling_utils.py:817] 2023-09-12 06:18:31,482 >> Model weights saved in /home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#  Pretrain Representation Models (Here we use Graphcodebert like sec 3.4)\n",
    "#preprocess\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'model/data/feature')\n",
    "outputPath=os.path.join(current,'model/data/output_static.txt')\n",
    "dir=os.path.join(current,'model/pretrainedModel/staticRepresentation')\n",
    "!cd $dir && python preprocess.py --data_path $path --output_path $outputPath\n",
    "#pretrain\n",
    "storedPath=os.path.join(os.getcwd(),'model/pretrainedModel/staticRepresentation/trainedModel')\n",
    "!cd $dir && python train.py --model_name_or_path graphcodebert-base --train_data_file $outputPath --per_device_train_batch_size 8 --do_train --output_dir $storedPath --mlm --overwrite_output_dir --line_by_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/CONCOCTION/model/pretrainedModel/staticRepresentation/trainedModel and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2573,  0.2688, -0.1238,  ..., -0.1123, -0.1479,  0.3731],\n",
      "         [ 0.5010, -0.2858,  0.6845,  ...,  0.2044,  0.8592,  0.0892],\n",
      "         [ 0.1018,  0.6084,  0.0398,  ...,  0.5097, -0.3028,  0.1666],\n",
      "         ...,\n",
      "         [ 0.2921,  0.1633,  0.7146,  ...,  0.7883,  0.5627, -0.1219],\n",
      "         [ 0.2624,  0.6262, -0.0925,  ...,  1.7611, -0.5254,  0.1670],\n",
      "         [ 0.2574,  0.2688, -0.1238,  ..., -0.1123, -0.1481,  0.3735]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (like the example in https://github.com/microsoft/CodeBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "storedPath=os.path.join(os.getcwd(),'model/pretrainedModel/staticRepresentation/trainedModel')\n",
    "tokenizer = AutoTokenizer.from_pretrained(storedPath)\n",
    "model = AutoModel.from_pretrained(storedPath)\n",
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.eos_token]\n",
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### *Dynamic representation model*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.678393300Z",
     "start_time": "2023-05-08T13:43:09.665390300Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  Extract Program Information (To show dynamic code information like sec 3.3.2.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION\n",
      "dirs: 1it [00:00, 25.94it/s]\n",
      "cpu\n",
      "device\n",
      "09/12/2023 06:19:28 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0 distributed training: False, 16-bits training: False\n",
      "09/12/2023 06:19:28 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='./result', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Sep12_06-19-28_5ab788861ade', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=2, dataloader_num_workers=0, past_index=-1, run_name='./result', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)\n",
      "09/12/2023 06:19:29 - WARNING - datasets.builder -   Using custom data configuration default-ea9df79bfef136d7\n",
      "Downloading and preparing dataset text/default to ./data/text/default-ea9df79bfef136d7/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4025.24it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 858.96it/s]\n",
      "Dataset text downloaded and prepared to ./data/text/default-ea9df79bfef136d7/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 361.14it/s]\n",
      "[INFO|configuration_utils.py:443] 2023-09-12 06:19:29,396 >> loading configuration file bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-12 06:19:29,398 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:443] 2023-09-12 06:19:29,400 >> loading configuration file bert-base-uncased/config.json\n",
      "[INFO|configuration_utils.py:481] 2023-09-12 06:19:29,402 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1766] 2023-09-12 06:19:49,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1766] 2023-09-12 06:19:49,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|modeling_utils.py:1025] 2023-09-12 06:19:49,536 >> loading weights file bert-base-uncased/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:1135] 2023-09-12 06:19:52,174 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1146] 2023-09-12 06:19:52,174 >> Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 119.34ba/s]\n",
      "[INFO|trainer.py:442] 2023-09-12 06:19:52,207 >> The following columns in the training set don't have a corresponding argument in `BertForCL.forward` and have been ignored: .\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -   ***** Running training *****\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Num examples = 2\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Num Epochs = 1\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Instantaneous batch size per device = 32\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Gradient Accumulation steps = 1\n",
      "09/12/2023 06:19:52 - INFO - utils.trainers -     Total optimization steps = 1\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/root/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.59it/s]09/12/2023 06:19:52 - INFO - utils.trainers -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 0.6343, 'train_samples_per_second': 1.577, 'epoch': 1.0}      \n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "[INFO|trainer.py:1344] 2023-09-12 06:19:52,849 >> Saving model checkpoint to ./result\n",
      "[INFO|configuration_utils.py:300] 2023-09-12 06:19:52,918 >> Configuration saved in ./result/config.json\n",
      "[INFO|modeling_utils.py:817] 2023-09-12 06:19:56,206 >> Model weights saved in ./result/pytorch_model.bin\n",
      "09/12/2023 06:19:56 - INFO - __main__ -   ***** Train results *****\n",
      "09/12/2023 06:19:56 - INFO - __main__ -     epoch = 1.0\n",
      "09/12/2023 06:19:56 - INFO - __main__ -     train_runtime = 0.6343\n",
      "09/12/2023 06:19:56 - INFO - __main__ -     train_samples_per_second = 1.577\n"
     ]
    }
   ],
   "source": [
    "#  Pretrain Representation Models (Here we use Simcse like sec 3.4)\n",
    "#preprocess\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'model/data/feature')\n",
    "outputPath=os.path.join(current,'model/data/output_dynamic.txt')\n",
    "dir=os.path.join(current,'model/pretrainedModel/dynamicRepresentation')\n",
    "!cd $dir && python preprocess.py --data_path $path --output_path $outputPath\n",
    "#training\n",
    "! cd $dir && python train.py --model_name_or_path bert-base-uncased     --train_file $outputPath   --output_dir ./result    --num_train_epochs 1     --per_device_train_batch_size 32     --learning_rate 3e-5     --max_seq_length 32      --metric_for_best_model stsb_spearman  --load_best_model_at_end     --eval_steps 2     --pooler_type cls     --mlp_only_train     --overwrite_output_dir     --temp 0.05     --do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/CONCOCTION/model/pretrainedModel/dynamicRepresentation/result and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert None to PyString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d7a5df8884a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcode_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"def max(a,b): if a>b: return a else return b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnl_tokens\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcode_tokens\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokens_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_token_to_id_with_added_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1/lib/python3.6/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_token_to_id_with_added_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert None to PyString"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (like the example in https://github.com/microsoft/CodeBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch,os\n",
    "storedPath=os.path.join(os.getcwd(),'model/pretrainedModel/dynamicRepresentation/result')\n",
    "tokenizer = AutoTokenizer.from_pretrained(storedPath)\n",
    "model = AutoModel.from_pretrained(storedPath)\n",
    "nl_tokens=tokenizer.tokenize(\"return maximum value\")\n",
    "code_tokens=tokenizer.tokenize(\"def max(a,b): if a>b: return a else return b\")\n",
    "tokens=[tokenizer.cls_token]+nl_tokens+[tokenizer.sep_token]+code_tokens+[tokenizer.eos_token]\n",
    "tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "context_embeddings=model(torch.tensor(tokens_ids)[None,:])[0]\n",
    "print(context_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2. Vulnerability Detection\n",
    "\n",
    " Concoction’s detection component takes the joint embedding as input to predict the presence of vulnerabilities.  Our current implementation only identifies whether a function may contain a vulnerability or bug and does not specify the type of vulnerability. Here we use SARD benchmarks.\n",
    "\n",
    "**approximate runtime ~ 30 minutes (please wait before moving to the next cell)**\n",
    "\n",
    "#### *Dynamic representation model*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.697397700Z",
     "start_time": "2023-05-08T13:43:09.679393300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Extract Program Information and Generate Joint Embedding (To show dynamic code information like sec 3.3 and sec 3.5.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION\n",
      "0\n",
      "2023-09-12 06:45:51,369 : Load data from exist npy\n",
      "[2023-09-12 06:45:51] INFO (root/MainThread) Load data from exist npy\n",
      "total train files is  146\n",
      "2023-09-12 06:45:51,529 : Computing embedding for train\n",
      "[2023-09-12 06:45:51] INFO (root/MainThread) Computing embedding for train\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:07<00:00,  2.52s/it]\n",
      "2023-09-12 06:45:59,099 : Computed train embeddings\n",
      "[2023-09-12 06:45:59] INFO (root/MainThread) Computed train embeddings\n",
      "2023-09-12 06:45:59,099 : Computing embedding for dev\n",
      "[2023-09-12 06:45:59] INFO (root/MainThread) Computing embedding for dev\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.49s/it]\n",
      "2023-09-12 06:46:01,588 : Computed dev embeddings\n",
      "[2023-09-12 06:46:01] INFO (root/MainThread) Computed dev embeddings\n",
      "2023-09-12 06:46:01,589 : Computing embedding for test\n",
      "[2023-09-12 06:46:01] INFO (root/MainThread) Computing embedding for test\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "2023-09-12 06:46:03,945 : Computed test embeddings\n",
      "[2023-09-12 06:46:03] INFO (root/MainThread) Computed test embeddings\n",
      "2023-09-12 06:46:03,945 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "[2023-09-12 06:46:03] INFO (root/MainThread) Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2023-09-12 06:46:04,021 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,039 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,058 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,077 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,095 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,114 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,132 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,151 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,151 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,189 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,208 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,226 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,244 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,263 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,281 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,299 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,318 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,318 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,342 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,360 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,378 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,397 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-12 06:46:04,415 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,434 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,452 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,470 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,470 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,494 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,513 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,531 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,549 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,568 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,586 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,604 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,623 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,623 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,627 : [('reg:1e-05', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.0001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.01', array([0.97959184, 0.95999998, 1.        , 0.9795918 ]))]\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) [('reg:1e-05', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.0001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.001', array([0.97959184, 0.95999998, 1.        , 0.9795918 ])), ('reg:0.01', array([0.97959184, 0.95999998, 1.        , 0.9795918 ]))]\n",
      "2023-09-12 06:46:04,629 : Validation : f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Validation : f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,629 : Evaluating...\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Evaluating...\n",
      "2023-09-12 06:46:04,648 : Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 10 epochs: f1 = 0.8679245546136568, precision = 0.7931034564971924,recall = 0.9583333134651184,accuracy = 0.8571428571428571\n",
      "2023-09-12 06:46:04,667 : Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 20 epochs: f1 = 0.6575342509598463, precision = 0.4897959232330322,recall = 1.0,accuracy = 0.4897959183673469\n",
      "2023-09-12 06:46:04,685 : Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 30 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,703 : Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 40 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,722 : Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 50 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,740 : Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 60 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,758 : Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 70 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,777 : Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training for 80 epochs: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,777 : Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Training best: f1 = 0.9795917957735685, precision = 0.9599999785423279,recall = 1.0,accuracy = 0.9795918367346939\n",
      "2023-09-12 06:46:04,778 : model clf saved to./f1_0.9795917957735685_2023-09-12.h5...\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) model clf saved to./f1_0.9795917957735685_2023-09-12.h5...\n",
      "2023-09-12 06:46:04,783 : Test : f1 = 0.8823529294205378, precision = 0.7894737124443054,recall = 1.0,accuracy = 0.9183673469387755\n",
      "[2023-09-12 06:46:04] INFO (root/MainThread) Test : f1 = 0.8823529294205378, precision = 0.7894737124443054,recall = 1.0,accuracy = 0.9183673469387755\n",
      "this is the data path: /home/model/data/sard/cwe416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#  Train detection Model (Here we use Simcse like sec 3.5.2)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'model/data/feature')\n",
    "dir=os.path.join(current,'model/detectionModel')\n",
    "!cd $dir && python evaluation_bug.py --path_to_data /home/model/data/sard/cwe416 --mode train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/CONCOCTION\n",
      "{'nhid': 2, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 200}\n",
      "0\n",
      "2023-09-12 06:46:41,392 : Load data from exist npy\n",
      "[2023-09-12 06:46:41] INFO (root/MainThread) Load data from exist npy\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#  Show the Trained Model (Load trained model and test on test case)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'model/data/feature')\n",
    "dir=os.path.join(current,'model/detectionModel')\n",
    "!cd $dir && python evaluation_bug.py  --model_to_load /home/CONCOCTION/model/detectionModel/f1_0.9795917957735685_2023-09-12.h5 --path_to_data $path  --mode test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Deployment\n",
    "\n",
    "This demo shows how to deploy our trained model on a real world project. Here we apply the xx as our test project.\n",
    "\n",
    "#### *Path Selection for Symbolic Execution*:\n",
    "After training the end-to-end model, we develop a path selection component to automatically select a subset of important paths whose dynamic traces are likely to improve prediction accuracy during deployment.\n",
    "\n",
    "*approximate runtime ~ 30 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.709400100Z",
     "start_time": "2023-05-08T13:43:09.695397900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execution path representation (shown as Sec. 3.6.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning for path selection (Sec. 3.6.2)\n",
    "#data preprocess\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "current=os.getcwd()\n",
    "path=os.path.join(current,'model/data/feature')\n",
    "storedDir=os.path.join(current,'model/data/feature_path')\n",
    "final_storedDir=os.path.join(current,'model/data/feature_path_text')\n",
    "dir=os.path.join(current,'model/pathSelection')\n",
    "if not os.path.exists(storedDir):\n",
    "    os.mkdir(storedDir)\n",
    "! cd $dir && python preprocess.py --data_path $path --stored_path $storedDir\n",
    "#select path\n",
    "!cd $dir --data_path $storedDir --stored_path $final_storedDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbolic execution for chosen paths (Sec. 3.6.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fuzzing for Test Case Generation*:\n",
    "\n",
    " We use fuzzing techniques to generate test cases for functions predicted to contain potential vulnerabilities, aiming to automate the testing process and minimize the need for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.727404100Z",
     "start_time": "2023-05-08T13:43:09.711400500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilizing AFL++ To Objective project (Shown as Sec. 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Experimental Evaluation\n",
    "\n",
    "Here, we provide a small-sized evaluation to showcase the working mechanism of Concoction bug detection. A full-scale evaluation, which takes more than a day to run, is provided through the Docker image (with detailed instructions on our project Github).\n",
    "\n",
    "### Large-scale Testing (Section 5.1)\n",
    "\n",
    " This part (add a web link) gives a quantified summary of Concoction for detecting function-level code vulnerabilities across the 20 projects listed in Table 1 in our papaer.\n",
    "\n",
    "This demo corresponds to Table 5 of the submitted manuscript.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Open Dataset (Section  5.2)\n",
    "\n",
    "We now evaluate our vulnerability detection model on the SARD and CVE datasets in Table 2 in paper.\n",
    "\n",
    "This demo corresponds to Figure 9 and 10 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = 10 minutes for one benchmark*\n",
    "\n",
    "#### *Sard Dataset*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.740861300Z",
     "start_time": "2023-05-08T13:43:09.725404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset and preprocess (use parameter to change CWE type and method,\n",
    "#                                     like FUN A( dataset = 'CWE-123', method = 'Vuldeepecker' ))\n",
    "\n",
    "# Train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Github Dataset*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.758882900Z",
     "start_time": "2023-05-08T13:43:09.741861500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset and preprocess (use parameter to method, like FUN A( dataset = 'Github', method = 'Vuldeepecker' ))\n",
    "\n",
    "# Train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Full-scale evaluation data\n",
    "\n",
    "We now plot the diagrams using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 9 and 10 (Section 5.2) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.772886Z",
     "start_time": "2023-05-08T13:43:09.757882600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 9: Evaluation on standard vulnerability databases. Min-max bars show performance across vulnerability types.\n",
    "# Output Figure 10:  Evaluation on the CVE dataset Concoction gives the best performance across evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study 3:  Evaluation on Opensource Projects (Section 5.3)\n",
    "\n",
    "We now compare to the baseline methods by applying them to the three open-source projects in Table 3 with a total of 35 CVEs reported by independent users.\n",
    "\n",
    "This demo corresponds to Figure 11 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = xx minutes for one benchmark*\n",
    "\n",
    "### Client RL Deployment Demo\n",
    "\n",
    "This demo shows how to apply the saved client RL to optimize a test program for Code Size Reduction. \n",
    "\n",
    "*approximate runtime ~ 15 minutes*\n",
    "\n",
    "#### Performance evaluation on benchmarks\n",
    "Benchmarks: Sqlite, Libtiff, Libpng.\n",
    "*approximate runtime ~ 20 minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.790890100Z",
     "start_time": "2023-05-08T13:43:09.773886200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and test benchmarks (Input benchmark and method like FUN A( project = 'Sqlite', method = 'Concoction')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-scale evaluation data\n",
    "We now generate the table using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 11 (Section 5.3) of the submitted manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.804893200Z",
     "start_time": "2023-05-08T13:43:09.788889500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 11: The number of vulnerabilities identified Concoction and other methods for open-source projects in Table 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Analysis (Alternative)\n",
    "\n",
    "####  DL model implementation choices.\n",
    "\n",
    "we evaluate several variants of Concoction on the CVE dataset.\n",
    "\n",
    "This demo corresponds to Figure 12 of the submitted manuscript.\n",
    "\n",
    "*approximate runtime = xx minutes for one benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.822897400Z",
     "start_time": "2023-05-08T13:43:09.805893300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and test CVE benchmarks (Input method like FUN A( method = 'Static')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full-scale evaluation data\n",
    "We now generate the figure using full-scale evaluation data (it would take too long to run the experiment lively). The results correspond to Figure 12 (Section 5.4) of the submitted manuscript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T13:43:09.835900200Z",
     "start_time": "2023-05-08T13:43:09.819896400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output Figure 12:  Comparing implementation variants of Concoction on the CVE dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7.1]",
   "language": "python",
   "name": "conda-env-pytorch1.7.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
